{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import rc\n",
    "from sklearn.datasets import make_blobs\n",
    "from random import sample\n",
    "from random import random\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para graficar a partir de una gráfica y la lista de thetas para la animación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animacion(fig, thetas):\n",
    "    x = np.linspace(ax.get_xlim()[0], ax.get_xlim()[1], 200)\n",
    "    def anima(i):            \n",
    "        y = thetas[i][0] +thetas[i][1] * x\n",
    "        line.set_data(x, y)\n",
    "        return line,\n",
    "    return animation.FuncAnimation(fig, anima, frames=len(thetas), interval=100, blit=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrón\n",
    "Dado un conjunto linealmente separable el algoritmo del perceptrón encuentra\n",
    "las thetas de la recta que permiten clasificar las observaciones a partir de un\n",
    "conjunto de entrenamiento.\n",
    "$\\\\ $\n",
    "$X \\in M_{MxN}$ donde $M$ es el número de observaciones y $N$ el número de características.$\\\\ $\n",
    "$y \\in R^M $ donde $y_i = 1$ si la observación $i$ pertenece a la clase o $y_i = -1$ si no.$\\\\ $\n",
    "$\\tau$ es el número de repeticiones para que el algoritmo converja\n",
    "\n"
   ]
  },
  {
   "source": [
    "def perceptron(X, y, tao = 100):\n",
    "    M, N =  X.shape # M - Filas, N - Columnas\n",
    "    theta = np.zeros(N)\n",
    "    theta0 = np.zeros(1)        \n",
    "\n",
    "    thetas = []\n",
    "    for i in range(tao):        \n",
    "        for j in range(M):\n",
    "            # Si es menor a 0 significa que está mal clasificado\n",
    "            if np.dot(y[j], np.dot(theta, X[j][:]) + theta0) <= 0:\n",
    "                theta = theta + y[j]*X[j][:]\n",
    "                theta0 = theta0 + y[j]                \n",
    "                thetas.append([-theta0/theta[1],-theta[0]/theta[1]])                \n",
    "        \n",
    "    return (theta, theta0, thetas)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X, y, opcion, perc = 0.8, lr = 0.01, lamb = 0.1):\n",
    "    n = int(len(y)*perc)    \n",
    "    M, N = X.shape\n",
    "\n",
    "    # Índices aleatorios para el conjunto de entrenamiento\n",
    "    index = sample([i for i in range(len(y))], n) \n",
    "    \n",
    "    #Filtramos el conjunto de entrenamiento\n",
    "    mask = np.zeros(len(y), dtype=bool)\n",
    "    mask[index] = True\n",
    "    trainingSetX = X[mask,]\n",
    "    trainingSetY = y[mask]        \n",
    "\n",
    "    #Filtramos el resto de los índices para el conjunto de evaluación  \n",
    "    mask = np.ones(len(y), dtype = bool)\n",
    "    mask[index] = False\n",
    "    testSetX = X[mask,]\n",
    "    testSetY = y[mask]\n",
    "    \n",
    "    # Dependiendo de la opción usa el algoritmo\n",
    "    if opcion == \"Perceptron\" : \n",
    "        theta, theta0, thetas = perceptron(trainingSetX, trainingSetY)   \n",
    "    elif opcion == \"SVM\":\n",
    "        theta, theta0, thetas = gradientDescentSVM(trainingSetX, trainingSetY, np.array([np.random.rand() for i in range (N)]), np.random.rand(), lr, lamb = lamb)\n",
    "    else:\n",
    "        thetaEst, thetasEst = gradientDescentEst(X, Y, learning_rate = lr, lamb = lamb)\n",
    "    \n",
    "    #Evaluamos el modelo con las thetas que nos regresó el modelo\n",
    "    correct = [0 if testSetY[i]*(np.dot(theta, testSetX[i,:])+theta0) <= 0 else 1 for i in range(len(testSetY))]\n",
    "    print(\"Accuracy of \" + str(sum(correct)/len(testSetY)*100) + \"%\")\n",
    "    return (theta, theta0, thetas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba de perceptrón"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "# Cargamos datos\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "# Filtramos entre setosa y versicolor\n",
    "X = X[Y<2, 0:2]\n",
    "Y = Y[Y<2]\n",
    "#Lo modificamos para que donde sea setosa Y = 1\n",
    "# y -1 en c. o. c. para que funcione el perceptrón\n",
    "Y = np.where(Y == 0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, theta0, thetas = perceptron(X,Y, tao = 200)"
   ]
  },
  {
   "source": [
    "# Animacióm del perceptrón"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "line, = ax.plot([], [], 'k-')\n",
    "\n",
    "ax.scatter(X[Y==1, 0], X[Y==1, 1], c='b', label=iris.target_names[0])\n",
    "ax.scatter(X[Y==-1, 0], X[Y==-1, 1], c='r', label=\"Not setosa\")\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "ax.set_xlabel(iris.feature_names[0])\n",
    "ax.set_ylabel(iris.feature_names[1])\n",
    "\n",
    "x = np.linspace(ax.get_xlim()[0], ax.get_xlim()[1], 200)\n",
    "ax.scatter(x, -theta0/theta[1] -theta[0]/theta[1]*x)\n",
    "\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "line, = ax.plot([], [], 'k-')\n",
    "\n",
    "ax.scatter(X[Y==1, 0], X[Y==1, 1], c='b', label=iris.target_names[0])\n",
    "ax.scatter(X[Y==-1, 0], X[Y==-1, 1], c='r', label=\"Not setosa\")\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "ax.set_xlabel(iris.feature_names[0])\n",
    "ax.set_ylabel(iris.feature_names[1])\n",
    "\n",
    "\n",
    "ax.grid(True)\n",
    "animado = animacion(fig, thetas)\n",
    "\n",
    "rc('animation', html='jshtml')\n",
    "animado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descenso por gradiente del clasificador lineal de máximo margen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de pérdida\n",
    "$ L_h(\\nu) = \\begin{cases} \n",
    "    \\text{1-v}& \\text{si }\\nu < 1 \\\\\n",
    "    \\text{0}&  c. o. c.\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hingeLoss(v):    \n",
    "    return(np.where(v<1, 1-v, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivada de la función de pérdida\n",
    "$ L_h(\\nu) = \\begin{cases} \n",
    "    \\text{-1}& \\text{si }\\nu < 1 \\\\\n",
    "    \\text{0}&  c. o. c.\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dHingeLoss(v):\n",
    "    return(np.where(v<1, -1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de costo\n",
    "$J(\\theta, \\theta_0) = \\frac{1}{n} \\displaystyle\\sum_{i = 1}^{n} Loss_h(y^{(i)}(\\theta\\cdot x^{(i)} + \\theta_0))$$+ \\frac{\\lambda}{2}\\lVert \\theta \\rVert^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(theta, theta0, X, y, lamb = 0.001):\n",
    "    n = len(y)\n",
    "    suma = 1/n*np.sum(hingeLoss(y*(np.dot(X, theta.transpose())+theta0))) + lamb/2*np.dot(theta, theta)\n",
    "    return suma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J(theta, theta0, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivada de la función de costo\n",
    "$\\nabla_\\theta J= \\frac{1}{n} \\displaystyle\\sum_{i = 1}^{n} Loss_h^\\prime(y^{(i)}(\\theta\\cdot x^{(i)} + \\theta_0))y^{(i)}x^{(i)}$$+ \\lambda \\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dJ(theta, theta0, X, y, lamb = 0.001):\n",
    "    n = len(y)\n",
    "    suma =  1/n*np.sum(np.dot(dHingeLoss(y*(np.dot(X, theta.transpose())+theta0)), y)*X, axis = 0) + lamb*theta\n",
    "    return suma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dJ(theta, theta0, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivada de la función de costo con respecto a $\\theta_0$\n",
    "$\\frac{\\partial J}{\\partial \\theta_0}= \\frac{1}{n} \\displaystyle\\sum_{i = 1}^{n} Loss_h^\\prime(y^{(i)}(\\theta\\cdot x^{(i)} + \\theta_0))y^{(i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dJ_0 (theta, theta0, X, y):\n",
    "    n = len(y)\n",
    "    suma =  1/n*np.sum(np.dot(dHingeLoss(y*(np.dot(X, theta.transpose())+theta0)), y))\n",
    "    return suma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dJ_0(theta, theta0, X, Y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentSVM(X, y, theta_, theta0_, learningRate, iter = 500, eps = 1e-6, lamb = 0.01):\n",
    "    theta = theta_\n",
    "    theta0 = theta0_\n",
    "    n = len(y)\n",
    "    t = 0   \n",
    "    thetas = []\n",
    "    # Mientras no converge o no alcanza el número máximo de iteraciones se mantiene ejecutándose\n",
    "    while True and t < iter:\n",
    "        t = t+1\n",
    "        thetaT = theta - learningRate*dJ(theta, theta0, X, y)\n",
    "        theta0T = theta0 - learningRate*dJ_0(theta, theta0, X, y)\n",
    "        if abs(J(theta, theta0, X, y)- J(thetaT, theta0T, X, y)) <  eps:\n",
    "            break\n",
    "        theta = thetaT\n",
    "        theta0 =  theta0T\n",
    "        #y = -theta0/theta[1] + (-theta[0]/theta[1])*x\n",
    "        thetas.append([-theta0/theta[1],-theta[0]/theta[1]])\n",
    "    return (thetaT, theta0T, thetas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaSVM, theta0SVM, thetasSVM = gradientDescentSVM(X, Y, theta , theta0 , 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "animado = animacion(fig, thetasSVM)\n",
    "\n",
    "rc('animation', html='jshtml')\n",
    "animado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descenso por gradiente estocástico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentEst(X, y, learning_rate, T = 1000, lamb = 0.01):\n",
    "    M,N = X.shape # M - Filas, N - Columnas\n",
    "    random.seed(1)\n",
    "    theta = np.array([np.random.rand() for i in range (N)])  \n",
    "    thetas = []\n",
    "    for t in range (T):\n",
    "        index = random.randint(0, M-1)             \n",
    "        theta = theta - learning_rate*((np.dot(theta.transpose(), X[index,:])-y[index:index+1])*X[index,:]+lamb/M*theta) \n",
    "        thetas.append([0, -theta[0]/theta[1]])            \n",
    "    return (theta, thetas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "thetaEst, thetasEst = gradientDescentEst(X,Y, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animado = animacion(fig, thetasEst[250:750])\n",
    "\n",
    "rc('animation', html='jshtml')\n",
    "animado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidation(X, y, k, lamb):\n",
    "    M, N =  X.shape # M - Filas, N - Columnas\n",
    "    size = int(M/k )\n",
    "    error = 0\n",
    "    theta = np.array([np.random.rand() for i in range (N)])\n",
    "    theta0 = np.random.rand()            \n",
    "    \n",
    "    for i in range(k):\n",
    "        leftX = X[0:size*i, :]\n",
    "        rightX = X[size*(i+1): M , :]     \n",
    "        leftY = y[0:size*i]\n",
    "        rightY = y[size*(i+1): M ]   \n",
    "        # gradientDescentSVM(X, y, theta_, theta0_, learningRate, iter = 200, eps = 1e-6, lamb = 0.01)\n",
    "        theta,theta0,t_ = gradientDescentSVM(np.concatenate((leftX, rightX)), np.concatenate((leftY, rightY)), theta, theta0, 0.01,           lamb = lamb)         \n",
    "        testingSetX = X[size*i:size*(i+1), :]\n",
    "        testingSetY = y[size*i:size*(i+1)]        \n",
    "        error += sum([1 if np.dot(y[j], np.dot(theta, X[j][:]) + theta0) <= 0 else 0 for j in range(size) ])    \n",
    "    return 1/k*error\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = crossValidation(X, Y, 5, 0.001)\n",
    "error\n"
   ]
  },
  {
   "source": [
    "# Prueba de los modelos y selección de lambda"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un conjunt del 80% par entrenar y 20% para probar\n",
    "n = int(len(Y)*0.8)    \n",
    "index = sample([i for i in range(len(Y))], n)     \n",
    "    \n",
    "mask = np.zeros(len(Y), dtype=bool)\n",
    "mask[index] = True\n",
    "trainingSetX = X[mask,]\n",
    "trainingSetY = Y[mask]        \n",
    "\n",
    "mask = np.ones(len(Y), dtype = bool)\n",
    "mask[index] = False\n",
    "testSetX = X[mask,]\n",
    "testSetY = Y[mask] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PELIGRO: NO CORRER ESTA CELDA, tarda bastante tiempo\n",
    "errores = []\n",
    "lambdas = []\n",
    "\n",
    "for i in range (1, 10000):\n",
    "    lambdas.append (1/i)\n",
    "    #errores.append (crossValidation(trainingSetX, trainingSetY, 5, 1/i))\n",
    "    errores.append (crossValidation(X, Y, 5, 1/i))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(errores, lambdas) \n",
    "\n",
    "plt.xlabel(\"Errores\")\n",
    "plt.ylabel(\"Lambdas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis \n",
    "Al observar la gráfica nos dimos cuenta que una lambda muy chica hacía que los errores fueran mayores que con una más grande.\n",
    "Nosotros pensamos que con una lambda muy pequeña hay overfitting al conjunto de entrenamiento y por eso hay un error más grande con el conjunto de prueba.\n",
    "En ese sentido, decidimos seleccionar una lambda pequeña $\\lambda = 0.1$, pero no tanto para evitar este fenomeno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomamos lambda = 0.1\n",
    "thetaSVM, theta0SVM, thetasSVM = test(X, Y, \"SVM\", lr = 0.001, lamb = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, theta0, thetas = test(X,Y, \"Perceptron\")"
   ]
  },
  {
   "source": [
    "# Conclusión\n",
    "Ambos modelos tienen en general una buena precisión, pero a veces es baja. Sin embargo, después de las diferentes pruebas, el método más estable es el del descenso por gradiente(SVM), pues al observar la animación nos damos cuenta que el Perceptrón varía mucho a veces cuando se corrigen las theta's.$\\\\ $\n",
    "Asimismo, el método que se nos hizo más interesante fue el descenso por gradiente estocástico. Lo anterior debido a que, a pesar de que en cada iteración solo corrige el modelo con una observación, al final converge a resultados considerablemente buenos.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}